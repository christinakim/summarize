{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/modelcard.json' to download model card file.\n",
      "Creating an empty model card.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'We introduce a simplemodification for autoencoder neural networks that yields powerful generative models. Our simple framework can be applied to multiple architectures, including deep ones. At test time, the method is significantly faster and scales better than other autoregressive estimators.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MADE arxiv abstract https://arxiv.org/abs/1502.03509\n",
    "text_to_summarize=\"\"\"Abstract: There has been a lot of recent interest in designing neural network models to\n",
    "estimate a distribution from a set of examples. We introduce a simple\n",
    "modification for autoencoder neural networks that yields powerful generative\n",
    "models. Our method masks the autoencoder's parameters to respect autoregressive\n",
    "constraints: each input is reconstructed only from previous inputs in a given\n",
    "ordering. Constrained this way, the autoencoder outputs can be interpreted as a\n",
    "set of conditional probabilities, and their product, the full joint\n",
    "probability. We can also train a single network that can decompose the joint\n",
    "probability in multiple different orderings. Our simple framework can be\n",
    "applied to multiple architectures, including deep ones. Vectorized\n",
    "implementations, such as on GPUs, are simple and fast. Experiments demonstrate\n",
    "that this approach is competitive with state-of-the-art tractable distribution\n",
    "estimators. At test time, the method is significantly faster and scales better\n",
    "than other autoregressive estimators.\"\"\"\n",
    "summarizer(text_to_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"Transfer learning's effectiveness comes from pre-training a model on unlabeled text data with a self-supervised task. After that, the model can be fine-tuned on smaller labeled datasets, often resulting in (far) better performance than training on the labeled data alone. With T5, we propose reframing all NLP tasks into a unified text-to-text-format.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t5 blog post - https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\n",
    "\n",
    "text_to_summarize=\"\"\"Over the past few years, transfer learning has led to a new wave of state-of-the-art results in natural language processing (NLP). Transfer learning's effectiveness comes from pre-training a model on abundantly-available unlabeled text data with a self-supervised task, such as language modeling or filling in missing words. After that, the model can be fine-tuned on smaller labeled datasets, often resulting in (far) better performance than training on the labeled data alone. The recent success of transfer learning was ignited in 2018 by GPT , ULMFiT , ELMo , and BERT , and 2019 saw the development of a huge diversity of new methods like XLNet , RoBERTa , ALBERT , Reformer , and MT-DNN . The rate of progress in the field has made it difficult to evaluate which improvements are most meaningful and how effective they are when combined. In “ Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ”, we present a large-scale empirical survey to determine which transfer learning techniques work best and apply these insights at scale to create a new model that we call the Text-To-Text Transfer Transformer (T5). We also introduce a new open-source pre-training dataset, called the Colossal Clean Crawled Corpus (C4). The T5 model, pre-trained on C4, achieves state-of-the-art results on many NLP benchmarks while being flexible enough to be fine-tuned to a variety of important downstream tasks. In order for our results to be extended and reproduced, we provide the code and pre-trained models , along with an easy-to-use Colab Notebook to help get started. With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself. An important ingredient for transfer learning is the unlabeled dataset used for pre-training. To accurately measure the effect of scaling up the amount of pre-training, one needs a dataset that is not only high quality and diverse, but also massive. Existing pre-training datasets don’t meet all three of these criteria — for example, text from Wikipedia is high quality, but uniform in style and relatively small for our purposes, while the Common Crawl web scrapes are enormous and highly diverse, but fairly low quality. To satisfy these requirements, we developed the Colossal Clean Crawled Corpus (C4), a cleaned version of Common Crawl that is two orders of magnitude larger than Wikipedia . Our cleaning process involved deduplication, discarding incomplete sentences, and removing offensive or noisy content. This filtering led to better results on downstream tasks, while the additional size allowed the model size to increase without overfitting during pre-training. C4 is available through TensorFlow Datasets . With the T5 text-to-text framework and the new pre-training dataset (C4), we surveyed the vast landscape of ideas and methods introduced for NLP transfer learning over the past few years. The full details of the investigation can be found in our paper, including experiments on: model architectures , where we found that encoder-decoder models generally outperformed \"decoder-only\" language models; pre-training objectives , where we confirmed that fill-in-the-blank-style denoising objectives (where the model is trained to recover missing words in the input) worked best and that the most important factor was the computational cost; unlabeled datasets , where we showed that training on in-domain data can be beneficial but that pre-training on smaller datasets can lead to detrimental overfitting; training strategies , where we found that multitask learning could be close to competitive with a pre-train-then-fine-tune approach but requires carefully choosing how often the model is trained on each task; and scale , where we compare scaling up the model size, the training time, and the number of ensembled models to determine how to make the best use of fixed compute power. To explore the current limits of transfer learning for NLP, we ran a final set of experiments where we combined all of the best methods from our systematic study and scaled up our approach with Google Cloud TPU accelerators . Our largest model had 11 billion parameters and achieved state-of-the-art on the GLUE , SuperGLUE , SQuAD , and CNN/Daily Mail benchmarks. One particularly exciting result was that we achieved a near-human score on the SuperGLUE natural language understanding benchmark, which was specifically designed to be difficult for machine learning models but easy for humans. T5 is flexible enough to be easily modified for application to many tasks beyond those considered in our paper, often with great success. Below, we apply T5 to two novel tasks: closed-book question answering and fill-in-the-blank text generation with variable-sized blanks. Closed-Book Question Answering One way to use the text-to-text framework is on reading comprehension problems, where the model is fed some context along with a question and is trained to find the question's answer from the context. For example, one might feed the model the text from the Wikipedia article about Hurricane Connie along with the question \" On what date did Hurricane Connie occur? \" The model would then be trained to find the date \"August 3rd, 1955\" in the article. In fact, we achieved state-of-the-art results on the Stanford Question Answering Dataset (SQuAD) with this approach. In our Colab demo and follow-up paper , we trained T5 to answer trivia questions in a more difficult \"closed-book\" setting, without access to any external knowledge . In other words, in order to answer a question T5 can only use knowledge stored in its parameters that it picked up during unsupervised pre-training. This can be considered a constrained form of open-domain question answering . T5 is surprisingly good at this task. The full 11-billion parameter model produces the exact text of the answer 50.1%, 37.4%, and 34.5% of the time on TriviaQA , WebQuestions , and Natural Questions , respectively.  To put these results in perspective, the T5 team went head-to-head with the model in a pub trivia challenge and lost! Try it yourself by clicking the animation below. Fill-in-the-Blank Text Generation Large language models like GPT-2 excel at generating very realistic looking-text since they are trained to predict what words come next after an input prompt. This has led to numerous creative applications like Talk To Transformer and the text-based game AI Dungeon . The pre-training objective used by T5 aligns more closely with a fill-in-the-blank task where the model predicts missing words within a corrupted piece of text. This objective is a generalization of the continuation task, since the “blanks” can appear at the end of the text as well. To make use of this objective, we created a new downstream task called sized fill-in-the-blank , where the model is asked to replace a blank with a specified number of words. For example, if we give the model the input “ I like to eat peanut butter and sandwiches, ” we would train it to fill in the blank with approximately 4 words. We fine-tuned T5 on this task using C4 and found the resulting outputs to be quite realistic. It’s especially fun to see how the model adjusts its predictions based on the requested size for the missing text. For example, with the input, “ I love peanut butter and _N_ sandwiches, ”  the outputs looked like: We are excited to see how people use our findings , code , and pre-trained models to help jump-start their projects. Check out the Colab Notebook to get started, and share how you use it with us on Twitter! This work has been a collaborative effort involving Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu, Karishma Malkan, Noah Fiedel, and Monica Dinculescu.\"\"\"\n",
    "summarizer(text_to_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Every good idea is the result of some mental motion. Name / categorize the mental motions that lead to creative ideas. Categorize ideas using some pattern or notion of similarity. Use the examples to install the pattern, and look for the pattern elsewhere. List the most powerful ideas. List your ideas to one place and prioritize them. Rank them by likelihood of success, cost of execution, or external factors.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Jason's demo about creativity - https://roamresearch.com/#/app/jason/page/MDb35OCGf\n",
    "text_to_summarize=\"\"\"\n",
    "#Creativity [[ March 31st, 2020 ]] #Life systems #Startup Ideas Systematizing Creativity Why: seemed like last week, after [[ Josh Albrecht ]] presented the [[ Final boss startup ideas checklist ]] , that people were struggling to come up with ideas. Here are some mental models and techniques for generating, playing with, stretching, and multiplying an idea. Go from the conceptual to the concrete. Once you've broken a concept into its parts, you can experiment with them individually, recombine them, reassembling them, etc. Questions What are all of the ways in which the concept is used? How is the way we use the concept misleading? What gives the concept its value? What is the simplest possible version of the concept? The most complex version? What are all of the definitions that exist? What is the concept often conflated with? What major assumptions does applying or using the concept make? When do these assumptions differ from reality? Try answering the above questions for the following concepts... Loneliness. Are we SURE we're all talking about the same thing? e.g. feeling misunderstood / no single best friend / unrequited love / being a minority - all suggest different solutions! Blog posts. The \"most inspiring possible idea\" etc For any prompt, set a timer for 10 minutes and come up with 10 ideas. The workhorse. Builds momentum and frees your mind to focus. Go meta: spend 10m coming up with ways to brainstorm! Example Tina, Bryden and I made repeated use of this to iteratively brainstorm for 1h on Friday . Our minds implicitly assume a solution space. Try explicitly playing with these assumptions and reshape that space. Questions Create resource constraints/excess (time, attention, money, assumptions, etc.) What would a supervillain do? Types of Thinker: what would I generate if I was a mathematician / philosopher / psychologist / economist What would Hufflepuff / Gryffindor / etc do? What would X friend / historical figure do? Example I've found Josh's \"What would you pay for right now\" very generative Take concepts and ask how they are related or how they can be combined. Try Recombination Hotseat: in a group of 3, two players throw out random concepts; the third generates a novel, interesting, or valuable composition of the two words thrown out, in real time. Questions What does a supervillian with only 5 hours a week do to finish the thing do? How would Nietzsche answer the Thiel questions? Examples If you combine \"evolution\" with \"ideas\", you might start asking how ideas multiply, what makes them \"fit\"... and you stumble onto the field of memetics. Example \"brainstorm playlist\": 5m: pick your favorite 10 trends from the Airtable 3 x 10m: spend 10m on each of your 3 favorite brainstorm prompts Compute the outer product of trends + ideas 3m: think of trend + idea evaluation criteria (simplest, most surprising combo, most likely to come to pass, etc) For each of your top 3 evaluation criteria: spend 10m identifying the top 5 trend + idea combinations. Every good idea is the result of some mental motion. If you watch yourself or others carefully, you can identify these motions and generalize them. Questions Listen abstractively to people speaking. Name / categorize the mental motions that lead to creative ideas. Categorize ideas using some pattern or notion of similarity. Use the examples to install the pattern, and look for the pattern elsewhere. Read creative authors, and analyze their conceptual style. Watch your own thought, especially when you’ve solved something or generated something new. Ask after the prompts that lead to that path of thought. List the most powerful ideas. Ask, how could I have come up with this, or seen this pattern for the first time? Ask, how can I see this pattern more often when it comes up, and how does that process of pattern recognition work? Often, by seeing many examples of a pattern, you mind starts to pick it up much more easily. So to see patterns that others haven’t seen, look at data that others haven’t looked at. Learn to see the incompleteness of things (frameworks, conceptual schemes, say checking for collective exhaustiveness, or searching for datapoints that don’t fit) and complete them (or rework the ontology entirely) Examples Michael Nielsen's idea to identify and study great papers + what makes them great. If you read between the lines, books like [[ Daily Rituals - Mason Curry ]] can help you understand the process behind history's great minds Gather your ideas to one place and prioritize them. Some ideas are below, but it's worth spending 5-10m thinking of your own. Questions Rank them by likelihood of success. Rank them by cost of execution. Rank them by simplicity. Rank them by externalities (positive or negative: learning, new friends/connections, reputation). Examples Josh's [[ Final boss startup ideas checklist ]] .\n",
    "\"\"\"\n",
    "summarizer(text_to_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
